#!/usr/bin/env python3
"""
Calculus RAG - Streamlit Web Interface

A beautiful web interface for the Calculus RAG system with proper LaTeX rendering.

Usage: streamlit run app.py
"""

import asyncio
import sys
from pathlib import Path

import streamlit as st

sys.path.insert(0, str(Path(__file__).parent / "src"))

from calculus_rag.config import get_settings
from calculus_rag.embeddings.ollama_embedder import OllamaEmbedder
from calculus_rag.llm.cloud_llm import CloudLLM
from calculus_rag.llm.model_router import ComplexityLevel, ModelRouter
from calculus_rag.llm.ollama_llm import OllamaLLM
from calculus_rag.rag.pipeline import RAGPipeline
from calculus_rag.retrieval.prerequisite_aware_retriever import PrerequisiteAwareRetriever
from calculus_rag.retrieval.retriever import Retriever
from calculus_rag.vectorstore.pgvector_store import PgVectorStore


# Page configuration
st.set_page_config(
    page_title="Calculus Tutor",
    page_icon="üßÆ",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Custom CSS for better math rendering
st.markdown(
    """
    <style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
    }
    .stAlert {
        margin-top: 1rem;
    }
    .math-content {
        font-size: 1.1rem;
        line-height: 1.8;
    }
    </style>
    """,
    unsafe_allow_html=True,
)


def get_or_create_eventloop():
    """Get or create an event loop for async operations."""
    try:
        return asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        return loop


@st.cache_data(ttl=300)  # Cache for 5 minutes
def get_knowledge_base_stats():
    """Get dynamic stats from the database."""
    import asyncpg

    async def fetch_stats():
        settings = get_settings()
        try:
            conn = await asyncpg.connect(settings.postgres_dsn)

            # Total chunks
            total = await conn.fetchval("SELECT COUNT(*) FROM calculus_knowledge")

            # Count by source type
            sources = await conn.fetch("""
                SELECT
                    CASE
                        WHEN metadata->>'source' LIKE '%.pdf' THEN 'pdf'
                        WHEN metadata->>'source' LIKE '%.md' THEN 'markdown'
                        ELSE 'other'
                    END as source_type,
                    COUNT(*) as count
                FROM calculus_knowledge
                GROUP BY source_type
            """)

            # Count unique sources
            unique_sources = await conn.fetchval("""
                SELECT COUNT(DISTINCT metadata->>'source') FROM calculus_knowledge
            """)

            await conn.close()

            pdf_count = 0
            md_count = 0
            for row in sources:
                if row['source_type'] == 'pdf':
                    pdf_count = row['count']
                elif row['source_type'] == 'markdown':
                    md_count = row['count']

            return {
                'total': total or 0,
                'pdf_chunks': pdf_count,
                'markdown_chunks': md_count,
                'unique_sources': unique_sources or 0,
            }
        except Exception:
            # Return defaults if DB not available
            return {
                'total': 0,
                'pdf_chunks': 0,
                'markdown_chunks': 0,
                'unique_sources': 0,
            }

    loop = get_or_create_eventloop()
    return loop.run_until_complete(fetch_stats())


def fix_latex_rendering(text: str) -> str:
    r"""
    Convert LaTeX delimiters from \[ \] to $$ $$ for Streamlit rendering.

    Also handles inline math delimiters \( \) to $ $.

    Args:
        text: Text containing LaTeX with \[ \] delimiters

    Returns:
        Text with $$ $$ delimiters for Streamlit
    """
    import re

    # Replace display math: [ ... ] -> $$ ... $$
    text = re.sub(r'\\\[', '$$', text)
    text = re.sub(r'\\\]', '$$', text)

    # Also handle bare [ ] that might be used for display math
    # But be careful not to replace actual brackets in text
    text = re.sub(r'(?<!\w)\[(?=\s*\\)', '$$', text)
    text = re.sub(r'(?<=\s)\](?!\w)', '$$', text)

    # Replace inline math: \( ... \) -> $ ... $
    text = re.sub(r'\\\(', '$', text)
    text = re.sub(r'\\\)', '$', text)

    return text


def preprocess_latex_input(text: str) -> str:
    """
    Clean up pasted LaTeX equations from various sources.

    Handles common issues when copying from PDFs, web pages, or LaTeX editors.
    Detects duplicated representations and extracts clean LaTeX.

    Args:
        text: Raw user input that may contain LaTeX

    Returns:
        Cleaned text with proper LaTeX formatting
    """
    import re

    # Remove zero-width characters and other invisible Unicode
    text = re.sub(r'[\u200b\u200c\u200d\ufeff\u00ad]', '', text)

    # If we detect LaTeX commands like \frac, \int, etc., try to extract just the LaTeX part
    # Pattern: duplicated text like "x2‚àí4x2‚àíx‚àí6f(x) = \frac{...} f(x)=x2‚àíx‚àí6x2‚àí4"
    # Keep only the LaTeX version

    # Find LaTeX expressions (text containing backslash commands)
    latex_pattern = r'\\(?:frac|int|sum|sqrt|lim|sin|cos|tan|log|ln)\{[^}]*\}'

    if re.search(latex_pattern, text):
        # Text contains LaTeX - try to clean up duplicates

        # Remove the corrupted non-LaTeX duplicates that appear before/after LaTeX
        # Pattern: variable definitions repeated like "f(x)=..." appearing multiple times
        # Keep the one with LaTeX

        # Remove sequences like "x2‚àí4x2‚àíx‚àí6" (corrupted fractions without proper formatting)
        # These are usually duplicates of the LaTeX version
        text = re.sub(r'([a-z])\(?([a-z])\)?=([a-z0-9])([¬≤¬≥¬π0-9])([‚àí\-+])([0-9]+)([a-z0-9])([¬≤¬≥¬π0-9])([‚àí\-+])([a-z])([‚àí\-+])([0-9]+)', '', text)

        # Remove trailing corrupted equation copies (after the LaTeX)
        # Pattern like "f(x)=x2‚àíx‚àí6x2‚àí4‚Äã" at the end
        text = re.sub(r'\s*[a-z]\([a-z]\)=[a-z0-9‚àí\-+]+‚Äã?\s*$', '', text)

        # Remove leading corrupted equation copies (before "Problem:" or the LaTeX)
        text = re.sub(r'^[a-z]\([a-z]\)=[a-z0-9‚àí\-+]+\s*(?=[a-z]\([a-z]\)\s*=\s*\\)', '', text)

    # Fix common Unicode math symbols to LaTeX
    unicode_to_latex = {
        '‚à´': r'\int ', '‚àë': r'\sum ', '‚àè': r'\prod ',
        '‚àö': r'\sqrt', '‚â§': r'\leq ', '‚â•': r'\geq ',
        '‚â†': r'\neq ', '‚âà': r'\approx ', '‚àû': r'\infty ',
        '¬±': r'\pm ', '√ó': r'\times ', '√∑': r'\div ',
        '‚àÇ': r'\partial ', '‚àÜ': r'\Delta ', 'œÄ': r'\pi ',
        'Œ±': r'\alpha ', 'Œ≤': r'\beta ', 'Œ≥': r'\gamma ',
        'Œ∏': r'\theta ', 'Œª': r'\lambda ', '‚Üí': r'\to ',
        '¬≤': '^2', '¬≥': '^3', '¬π': '^1',
        '‚àí': '-',  # Unicode minus to regular minus
    }

    for unicode_char, latex_cmd in unicode_to_latex.items():
        text = text.replace(unicode_char, latex_cmd)

    # Clean up excessive whitespace
    text = re.sub(r' +', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)

    return text.strip()


@st.cache_resource
def initialize_rag_system():
    """Initialize the RAG system (cached to avoid re-initialization)."""
    settings = get_settings()

    # Load embedder
    embedder = OllamaEmbedder(
        model=settings.embedding_model_name,
        base_url=settings.ollama_base_url,
        dimension=settings.vector_dimension,
    )

    # Initialize vector store with new event loop
    async def init_vectorstore():
        vector_store = PgVectorStore(
            connection_string=settings.postgres_dsn,
            dimension=settings.vector_dimension,
            table_name="calculus_knowledge",
        )
        await vector_store.initialize()
        return vector_store

    loop = get_or_create_eventloop()
    vector_store = loop.run_until_complete(init_vectorstore())

    # Initialize Smart Model Router
    small_llm = OllamaLLM(
        model="qwen2-math:1.5b",
        base_url=settings.ollama_base_url,
        timeout=settings.ollama_request_timeout,
    )

    router = ModelRouter(enable_fallback=True)
    router.add_model(
        llm=small_llm,
        name="Fast-1.5B",
        max_complexity=ComplexityLevel.MODERATE,
    )

    # Use cloud LLM for complex queries if enabled (avoids heavy local 7B)
    if settings.cloud_llm_enabled and settings.cloud_llm_api_key:
        if settings.cloud_llm_provider == "ollama-cloud":
            # Ollama cloud models run through local Ollama server
            # Auth is handled via Ollama OAuth (user signs in via browser)
            cloud_llm = OllamaLLM(
                model=settings.cloud_llm_model,  # e.g., "deepseek-v3.1:671b-cloud"
                base_url=settings.ollama_base_url,  # Uses local Ollama
                timeout=settings.cloud_llm_timeout,
                # No API key needed - Ollama uses OAuth session
            )
        else:
            # OpenRouter or DeepSeek direct API
            cloud_llm = CloudLLM(
                api_key=settings.cloud_llm_api_key,
                model=settings.cloud_llm_model,
                provider=settings.cloud_llm_provider,
                timeout=settings.cloud_llm_timeout,
            )
        router.add_model(
            llm=cloud_llm,
            name=f"Cloud-{settings.cloud_llm_model.split('/')[-1].split(':')[0]}",
            max_complexity=ComplexityLevel.COMPLEX,
            is_fallback=True,
        )
    else:
        # Fallback to local 7B if cloud is not configured
        large_llm = OllamaLLM(
            model="qwen2-math:7b",
            base_url=settings.ollama_base_url,
            timeout=600,
        )
        router.add_model(
            llm=large_llm,
            name="Powerful-7B",
            max_complexity=ComplexityLevel.COMPLEX,
            is_fallback=True,
        )

    # Create retrievers
    retriever = Retriever(embedder=embedder, vector_store=vector_store)

    # Create prerequisite-aware retriever with hybrid search for enhanced context
    prereq_retriever = PrerequisiteAwareRetriever(
        embedder=embedder,
        vector_store=vector_store,
        max_prerequisite_depth=2,  # Include prereqs and their prereqs
        prerequisite_weight=0.8,   # Slightly lower weight for prereq content
        use_hybrid_search=True,    # Enable hybrid (semantic + keyword) search
        semantic_weight=0.7,       # 70% semantic, 30% keyword
    )

    # Create RAG pipeline with prerequisite-aware retrieval
    rag_pipeline = RAGPipeline(
        retriever=retriever,
        llm=router,
        n_retrieved_chunks=3,
        prerequisite_aware_retriever=prereq_retriever,
        use_prerequisite_retrieval=True,
    )

    return rag_pipeline, router, vector_store, loop


def query_rag_sync(rag_pipeline, question, temperature, loop, conversation_history=None):
    """Query the RAG system synchronously (wrapper for async)."""
    async def _query():
        return await rag_pipeline.query(
            question=question,
            temperature=temperature,
            conversation_history=conversation_history,
        )

    return loop.run_until_complete(_query())


def main():
    """Main Streamlit application."""

    # Header
    st.markdown('<div class="main-header">üßÆ Calculus Tutor</div>', unsafe_allow_html=True)
    st.markdown(
        """
        <div style="text-align: center; color: #666; margin-bottom: 2rem;">
        Your AI-powered calculus learning assistant with intelligent prerequisite support
        </div>
        """,
        unsafe_allow_html=True,
    )

    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è Settings")

        temperature = st.slider(
            "Response Creativity",
            min_value=0.0,
            max_value=1.0,
            value=0.3,
            step=0.1,
            help="Lower = more focused, Higher = more creative",
        )

        st.divider()

        st.header("üìö Knowledge Base")
        # Get dynamic stats from database
        stats = get_knowledge_base_stats()
        if stats['total'] > 0:
            st.info(
                f"""
                **{stats['unique_sources']} Sources Loaded:**
                - Paul's Online Notes (Algebra, Calculus)
                - Calculus Cheat Sheets & Practice Problems
                - Khan Academy Video Summaries
                - Study Guides & Reference Materials

                **Total:** {stats['total']:,} chunks
                """
            )
        else:
            st.warning("‚ö†Ô∏è Knowledge base not loaded. Run ingestion first.")

        st.divider()

        st.header("ü§ñ Smart Routing")
        # Show routing info based on configuration
        settings_check = get_settings()
        if settings_check.cloud_llm_enabled and settings_check.cloud_llm_api_key:
            st.success(
                f"""
                **Fast Model:** qwen2-math:1.5b
                - Simple questions
                - Quick responses

                **Cloud Model:** {settings_check.cloud_llm_model}
                - Complex proofs
                - Detailed explanations
                - No local resource usage
                """
            )
        else:
            st.success(
                """
                **Fast Model:** qwen2-math:1.5b
                - Simple questions
                - Quick responses

                **Powerful Model:** qwen2-math:7b
                - Complex proofs
                - Detailed explanations
                """
            )

        st.divider()

        st.header("üîç Retrieval")
        st.info(
            """
            **Hybrid Search:** Enabled
            - 70% Semantic (meaning)
            - 30% Keyword (BM25)

            **Prerequisite-Aware:** Active
            - Detects topic from query
            - Fetches related foundations
            """
        )

        st.divider()

        st.header("üí¨ Conversation")
        msg_count = len(st.session_state.get("messages", []))
        if msg_count > 0:
            st.success(f"**Memory Active:** {msg_count} messages\n\nI remember our conversation!")
        else:
            st.info("Start chatting - I'll remember context!")

        st.divider()

        st.header("üí° Example Questions")
        st.caption("Click any question to try it:")

        # Organized by difficulty/topic
        example_categories = {
            "üìó Basics": [
                "What is a derivative?",
                "Explain limits with an example",
            ],
            "üìò How-To": [
                "How do I use the chain rule?",
                "How do I integrate by parts?",
            ],
            "üìô Problem Solving": [
                "Find the derivative of sin(x¬≤)",
                "Evaluate the limit of (x¬≤-1)/(x-1) as x‚Üí1",
            ],
            "üìï Conceptual": [
                "Why does the derivative of eÀ£ equal eÀ£?",
                "What's the relationship between derivatives and integrals?",
            ],
        }

        for category, questions in example_categories.items():
            st.markdown(f"**{category}**")
            for q in questions:
                if st.button(q, key=f"ex_{hash(q)}", use_container_width=True):
                    st.session_state.example_question = q

        st.divider()

        st.header("üìù Paste Equation")
        with st.expander("Paste messy equation here to clean up"):
            st.markdown("**Paste your equation below, then click Clean & Copy:**")
            messy_input = st.text_area(
                "Paste equation:",
                height=80,
                placeholder="Paste messy LaTeX/equation here...",
                key="messy_equation",
            )
            col1, col2 = st.columns(2)
            with col1:
                if st.button("üßπ Clean & Show", use_container_width=True):
                    if messy_input:
                        cleaned = preprocess_latex_input(messy_input)
                        st.session_state.cleaned_equation = cleaned
            with col2:
                if st.button("üì§ Send to Chat", use_container_width=True):
                    if messy_input:
                        cleaned = preprocess_latex_input(messy_input)
                        st.session_state.pending_question = cleaned

            if "cleaned_equation" in st.session_state:
                st.code(st.session_state.cleaned_equation, language=None)

            st.markdown(
                """
                ---
                **Tips for writing equations:**
                - Just type naturally: "What is the integral of x¬≤?"
                - Unicode symbols (‚à´, ‚àë, œÄ) are auto-converted
                - Use LaTeX: `\\frac{x}{y}`, `\\int x^2 dx`
                """
            )

    # Initialize session state
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "rag_system" not in st.session_state:
        with st.spinner("üîß Loading RAG system... (this may take a moment)"):
            (
                st.session_state.rag_system,
                st.session_state.router,
                st.session_state.vector_store,
                st.session_state.event_loop,
            ) = initialize_rag_system()
        st.success("‚úÖ RAG system loaded!")

    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["role"] == "assistant":
                # Fix LaTeX rendering and display
                fixed_content = fix_latex_rendering(message["content"])
                st.markdown(fixed_content, unsafe_allow_html=True)
                if "sources" in message:
                    with st.expander("üìñ View Sources"):
                        for i, source in enumerate(message["sources"], 1):
                            is_prereq = source.get("is_prerequisite", False)
                            prereq_badge = " üìö *prerequisite*" if is_prereq else ""
                            st.caption(
                                f"**[{i}]** {source['pdf']} (relevance: {source['score']:.2f}) - {source['category']}{prereq_badge}"
                            )
                # Display model and topic info
                info_parts = []
                if "model" in message:
                    info_parts.append(f"ü§ñ Model: {message['model']}")
                if message.get("detected_topic"):
                    info_parts.append(f"üìç Topic: {message['detected_topic']}")
                if message.get("prerequisites_used"):
                    info_parts.append(f"üìö Prerequisites: {', '.join(message['prerequisites_used'])}")
                if info_parts:
                    st.caption(" | ".join(info_parts))
            else:
                st.markdown(message["content"])

    # Handle example question clicks
    if "example_question" in st.session_state:
        question = st.session_state.example_question
        del st.session_state.example_question
        st.session_state.messages.append({"role": "user", "content": question})
        st.rerun()

    # Handle pending question from equation cleaner
    if "pending_question" in st.session_state:
        question = st.session_state.pending_question
        del st.session_state.pending_question
        if "cleaned_equation" in st.session_state:
            del st.session_state.cleaned_equation
        st.session_state.messages.append({"role": "user", "content": question})
        st.rerun()

    # Chat input
    if prompt := st.chat_input("Ask a calculus question..."):
        # Preprocess LaTeX input (clean up pasted equations)
        cleaned_prompt = preprocess_latex_input(prompt)

        # Add user message to chat
        st.session_state.messages.append({"role": "user", "content": cleaned_prompt})

        with st.chat_message("user"):
            st.markdown(cleaned_prompt)

        # Generate response
        with st.chat_message("assistant"):
            with st.spinner("ü§î Thinking..."):
                # Build conversation history (exclude current message)
                history = [
                    {"role": m["role"], "content": m["content"]}
                    for m in st.session_state.messages[:-1]  # All except current
                ]

                # Query RAG system with conversation context
                response = query_rag_sync(
                    st.session_state.rag_system,
                    cleaned_prompt,
                    temperature,
                    st.session_state.event_loop,
                    conversation_history=history if history else None,
                )

                # Get model used
                model_used = st.session_state.router.last_model_used

                # Fix LaTeX rendering and display answer
                fixed_answer = fix_latex_rendering(response.answer)
                st.markdown(fixed_answer, unsafe_allow_html=True)

                # Display sources with prerequisite info
                if response.sources:
                    with st.expander("üìñ View Sources"):
                        for i, source in enumerate(response.sources, 1):
                            pdf_name = source.metadata.get("source", "Unknown")
                            score = source.score
                            category = source.metadata.get("category", "")
                            is_prereq = source.metadata.get("is_prerequisite", False)
                            prereq_badge = " üìö *prerequisite*" if is_prereq else ""
                            st.caption(
                                f"**[{i}]** {pdf_name} (relevance: {score:.2f}) - {category}{prereq_badge}"
                            )

                # Display topic and prerequisite info
                info_parts = [f"ü§ñ Model: {model_used}"]
                if response.detected_topic:
                    info_parts.append(f"üìç Topic: {response.detected_topic}")
                if response.prerequisites_used:
                    info_parts.append(f"üìö Prerequisites: {', '.join(response.prerequisites_used)}")
                st.caption(" | ".join(info_parts))

                # Save assistant response
                sources_info = [
                    {
                        "pdf": source.metadata.get("source", "Unknown"),
                        "score": source.score,
                        "category": source.metadata.get("category", ""),
                        "is_prerequisite": source.metadata.get("is_prerequisite", False),
                    }
                    for source in response.sources
                ]
                st.session_state.messages.append(
                    {
                        "role": "assistant",
                        "content": response.answer,
                        "sources": sources_info,
                        "model": model_used,
                        "detected_topic": response.detected_topic,
                        "prerequisites_used": response.prerequisites_used,
                    }
                )

    # Footer
    st.divider()
    col1, col2, col3 = st.columns(3)
    with col1:
        st.caption("üìä Questions asked: " + str(len([m for m in st.session_state.messages if m["role"] == "user"])))
    with col2:
        if st.button("üóëÔ∏è Clear Chat", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
    with col3:
        st.caption("üí° Powered by Qwen2-Math + Hybrid Search (pgvector + BM25)")


if __name__ == "__main__":
    main()
